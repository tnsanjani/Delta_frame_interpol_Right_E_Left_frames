{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c452864",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    for p in module.parameters():\n",
    "        nn.init.zeros_(p)\n",
    "    return module\n",
    "\n",
    "class ControlNetConditioningEmbeddingSVD(nn.Module):\n",
    "    \"\"\"\n",
    "    Quoting from https://arxiv.org/abs/2302.05543: \"Stable Diffusion uses a pre-processing method similar to VQ-GAN\n",
    "    [11] to convert the entire dataset of 512 × 512 images into smaller 64 × 64 “latent images” for stabilized\n",
    "    training. This requires ControlNets to convert image-based conditions to 64 × 64 feature space to match the\n",
    "    convolution size. We use a tiny network E(·) of four convolution layers with 4 × 4 kernels and 2 × 2 strides\n",
    "    (activated by ReLU, channels are 16, 32, 64, 128, initialized with Gaussian weights, trained jointly with the full\n",
    "    model) to encode image-space conditions ... into feature maps ...\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        conditioning_embedding_channels: int,\n",
    "        conditioning_channels: int = 1,\n",
    "        block_out_channels: Tuple[int, ...] = (32, 64, 128, 256),\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        \n",
    "        self.conv_in = nn.Conv2d(conditioning_channels, block_out_channels[0], kernel_size=3, padding=1)\n",
    "\n",
    "        self.blocks = nn.ModuleList([])\n",
    "\n",
    "        for i in range(len(block_out_channels) - 1):\n",
    "            channel_in = block_out_channels[i]\n",
    "            channel_out = block_out_channels[i + 1]\n",
    "            self.blocks.append(nn.Conv2d(channel_in, channel_in, kernel_size=3, padding=1))\n",
    "            self.blocks.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, padding=1, stride=2))\n",
    "\n",
    "        self.conv_out = zero_module(\n",
    "            nn.Conv2d(block_out_channels[-1], conditioning_embedding_channels, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, conditioning):\n",
    "        #this seeems appropriate? idk if i should be applying a more complex setup to handle the frames\n",
    "        #combine batch and frames dimensions\n",
    "        batch_size, frames, channels, height, width = conditioning.size()\n",
    "        conditioning = conditioning.view(batch_size * frames, channels, height, width)\n",
    "\n",
    "        embedding = self.conv_in(conditioning)\n",
    "        embedding = F.silu(embedding)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            embedding = block(embedding)\n",
    "            embedding = F.silu(embedding)\n",
    "\n",
    "        embedding = self.conv_out(embedding)\n",
    "        \n",
    "        #split them apart again\n",
    "        #actually not needed\n",
    "        #new_channels, new_height, new_width = embedding.shape[1], embedding.shape[2], embedding.shape[3]\n",
    "        #embedding = embedding.view(batch_size, frames, new_channels, new_height, new_width)\n",
    "\n",
    "\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2d3754d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 5, 6, 375, 375])\n",
      "Shape after resize: torch.Size([1, 5, 6, 320, 512])\n",
      "torch.Size([5, 320, 40, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange # Make sure to import this\n",
    "\n",
    "# --- Your Setup ---\n",
    "# Assuming controlnet_cond_embedding is a defined nn.Module\n",
    "# def controlnet_cond_embedding(x): return x # Example placeholder\n",
    "\n",
    "# Your initial tensor\n",
    "controlnet_cond = torch.rand([1, 5, 6, 375, 375])\n",
    "B, T = controlnet_cond.shape[0], controlnet_cond.shape[1] # Get Batch and Time\n",
    "target_h, target_w = 320, 512\n",
    "\n",
    "print(f\"Original shape: {controlnet_cond.shape}\")\n",
    "\n",
    "# --- Start Correction ---\n",
    "# 1. Flatten Batch and Time dimensions for 2D interpolation:\n",
    "# Shape changes from [B, T, C, H, W] -> [B*T, C, H, W]\n",
    "controlnet_cond_flat = rearrange(controlnet_cond, 'b t c h w -> (b t) c h w')\n",
    "\n",
    "# 2. Apply 2D spatial interpolation\n",
    "controlnet_cond_resized = F.interpolate(\n",
    "    controlnet_cond_flat,\n",
    "    size=(target_h, target_w), # Target (H, W)\n",
    "    mode='bilinear',          # Use bilinear interpolation\n",
    "    align_corners=False       # Recommended setting\n",
    ")\n",
    "\n",
    "# 3. Reshape back to 5D:\n",
    "# Shape changes from [B*T, C, H_new, W_new] -> [B, T, C, H_new, W_new]\n",
    "controlnet_cond = rearrange(controlnet_cond_resized, '(b t) c h w -> b t c h w', b=B)\n",
    "# --- End Correction ---\n",
    "\n",
    "print(f\"Shape after resize: {controlnet_cond.shape}\")\n",
    "\n",
    "\n",
    "conditioning_embedding_out_channels : Optional[Tuple[int, ...]] = (32, 64, 128, 256)\n",
    "controlnet_cond_embedding = ControlNetConditioningEmbeddingSVD(\n",
    "            conditioning_embedding_channels=320,\n",
    "            block_out_channels=conditioning_embedding_out_channels,\n",
    "            conditioning_channels=6)\n",
    "\n",
    "controlnet_cond = controlnet_cond_embedding(controlnet_cond)\n",
    "print(controlnet_cond.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af6a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 320, 47, 47])\n"
     ]
    }
   ],
   "source": [
    "conditioning_embedding_out_channels : Optional[Tuple[int, ...]] = (32, 64, 128, 256)\n",
    "controlnet_cond_embedding = ControlNetConditioningEmbeddingSVD(\n",
    "            conditioning_embedding_channels=320,\n",
    "            block_out_channels=conditioning_embedding_out_channels,\n",
    "            conditioning_channels=6)\n",
    "\n",
    "controlnet_cond = controlnet_cond_embedding(controlnet_cond)\n",
    "print(controlnet_cond.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "195d8803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8, 46, 46])\n",
      "torch.Size([5, 320, 46, 46])\n",
      "Original 'sample' shape:     torch.Size([5, 320, 46, 46])\n",
      "Original 'events' shape: torch.Size([5, 320, 40, 64])\n",
      "Resized 'sample' shape:    torch.Size([5, 320, 40, 64])\n",
      "Combined shape:          torch.Size([5, 320, 40, 64])\n"
     ]
    }
   ],
   "source": [
    "conv_in = nn.Conv2d(\n",
    "    8,\n",
    "    320,\n",
    "    kernel_size=3,\n",
    "    padding=1,\n",
    ")\n",
    "input = torch.rand(([1, 5, 8, 46, 46]))\n",
    "sample = input.flatten(0, 1)\n",
    "print(sample.shape)\n",
    "k = conv_in(sample)\n",
    "print(k.shape)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Your two tensors with mismatched H/W\n",
    "sample = torch.randn(5, 320, 46, 46)\n",
    "events_condition = torch.randn(5, 320, 40, 64)\n",
    "\n",
    "print(f\"Original 'sample' shape:     {sample.shape}\")\n",
    "print(f\"Original 'events' shape: {events_condition.shape}\")\n",
    "\n",
    "# 1. Get the target H/W from the 'events_condition' tensor\n",
    "target_height = events_condition.shape[2]  # 40\n",
    "target_width = events_condition.shape[3]   # 64\n",
    "\n",
    "# 2. Resize 'sample' to match the target size\n",
    "sample_resized = F.interpolate(\n",
    "    sample, \n",
    "    size=(target_height, target_width), # (40, 64)\n",
    "    mode='bilinear', \n",
    "    align_corners=False\n",
    ")\n",
    "\n",
    "print(f\"Resized 'sample' shape:    {sample_resized.shape}\")\n",
    "\n",
    "# Now they are the same size and can be combined\n",
    "combined_tensor = sample_resized + events_condition\n",
    "print(f\"Combined shape:          {combined_tensor.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VDM_EVFI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
